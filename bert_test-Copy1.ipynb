{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from string import punctuation\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Word</th>\n",
       "      <th>V.Mean.Sum</th>\n",
       "      <th>V.SD.Sum</th>\n",
       "      <th>V.Rat.Sum</th>\n",
       "      <th>A.Mean.Sum</th>\n",
       "      <th>A.SD.Sum</th>\n",
       "      <th>A.Rat.Sum</th>\n",
       "      <th>D.Mean.Sum</th>\n",
       "      <th>D.SD.Sum</th>\n",
       "      <th>...</th>\n",
       "      <th>A.Rat.L</th>\n",
       "      <th>A.Mean.H</th>\n",
       "      <th>A.SD.H</th>\n",
       "      <th>A.Rat.H</th>\n",
       "      <th>D.Mean.L</th>\n",
       "      <th>D.SD.L</th>\n",
       "      <th>D.Rat.L</th>\n",
       "      <th>D.Mean.H</th>\n",
       "      <th>D.SD.H</th>\n",
       "      <th>D.Rat.H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>19</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.40</td>\n",
       "      <td>22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.75</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.29</td>\n",
       "      <td>11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>8</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>22</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.09</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.74</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.64</td>\n",
       "      <td>21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.81</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.63</td>\n",
       "      <td>7</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.45</td>\n",
       "      <td>16</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.70</td>\n",
       "      <td>20</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.02</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>11</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.18</td>\n",
       "      <td>18</td>\n",
       "      <td>5.43</td>\n",
       "      <td>1.62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         Word  V.Mean.Sum  V.SD.Sum  V.Rat.Sum  A.Mean.Sum  \\\n",
       "0           1     aardvark        6.26      2.21         19        2.41   \n",
       "1           2      abalone        5.30      1.59         20        2.65   \n",
       "2           3      abandon        2.84      1.54         19        3.73   \n",
       "3           4  abandonment        2.63      1.74         19        4.95   \n",
       "4           5        abbey        5.85      1.69         20        2.20   \n",
       "\n",
       "   A.SD.Sum  A.Rat.Sum  D.Mean.Sum  D.SD.Sum  ...  A.Rat.L  A.Mean.H  A.SD.H  \\\n",
       "0      1.40         22        4.27      1.75  ...       11      2.55    1.29   \n",
       "1      1.90         20        4.95      1.79  ...       12      2.38    1.92   \n",
       "2      2.43         22        3.32      2.50  ...       11      3.82    2.14   \n",
       "3      2.64         21        2.64      1.81  ...       14      5.29    2.63   \n",
       "4      1.70         20        5.00      2.02  ...        9      2.55    1.92   \n",
       "\n",
       "   A.Rat.H  D.Mean.L  D.SD.L  D.Rat.L  D.Mean.H  D.SD.H  D.Rat.H  \n",
       "0       11      4.12    1.64        8      4.43    1.99        7  \n",
       "1        8      5.55    2.21       11      4.36    1.03       11  \n",
       "2       11      2.77    2.09       13      4.11    2.93        9  \n",
       "3        7      2.31    1.45       16      3.08    2.19       12  \n",
       "4       11      4.83    2.18       18      5.43    1.62        7  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('14k_words_VAD.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence max value: 8.53 and min value: 1.26\n",
      "Arousal max value: 7.79 and min value: 1.6\n"
     ]
    }
   ],
   "source": [
    "print('Valence max value:', max(df['V.Mean.Sum']), 'and min value:', min(df['V.Mean.Sum']))\n",
    "print('Arousal max value:', max(df['A.Mean.Sum']), 'and min value:', min(df['A.Mean.Sum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of happy words: 210\n",
      "Some examples: accomplishment, admiration, amazing\n"
     ]
    }
   ],
   "source": [
    "high_valence_words = df[df['V.Mean.Sum'] > max(df['V.Mean.Sum']) - 1]\n",
    "print('Numer of happy words:', len(high_valence_words))\n",
    "print('Some examples: ' + high_valence_words.iloc[1]['Word'], high_valence_words.iloc[3]['Word'], high_valence_words.iloc[6]['Word'], sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of happy words: 215\n",
      "Some examples: abduction, acne, alcoholism\n"
     ]
    }
   ],
   "source": [
    "low_valence_words = df[df['V.Mean.Sum'] < min(df['V.Mean.Sum']) + 1]\n",
    "print('Numer of happy words:', len(low_valence_words))\n",
    "print('Some examples: ' + low_valence_words.iloc[0]['Word'], low_valence_words.iloc[3]['Word'], low_valence_words.iloc[6]['Word'], sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(low_valence_words['A.Mean.Sum']))\n",
    "sad_from_val = low_valence_words[low_valence_words['A.Mean.Sum'] < 5.35]\n",
    "angry_from_val = low_valence_words[low_valence_words['A.Mean.Sum'] > 5.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(high_valence_words['A.Mean.Sum']))\n",
    "relaxed_from_val = high_valence_words[high_valence_words['A.Mean.Sum'] < 4.98]\n",
    "happy_from_val = high_valence_words[high_valence_words['A.Mean.Sum'] > 4.98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of happy words: 74\n",
      "Some examples: alarm, arsonist, attack\n"
     ]
    }
   ],
   "source": [
    "high_arousal_words = df[df['A.Mean.Sum'] > max(df['A.Mean.Sum']) - 1]\n",
    "print('Numer of happy words:', len(high_arousal_words))\n",
    "print('Some examples: ' + high_arousal_words.iloc[0]['Word'], high_arousal_words.iloc[3]['Word'], high_arousal_words.iloc[6]['Word'], sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of happy words: 204\n",
      "Some examples: abbey, area, axle\n"
     ]
    }
   ],
   "source": [
    "low_arousal_words = df[df['A.Mean.Sum'] < min(df['A.Mean.Sum']) + 1]\n",
    "print('Numer of happy words:', len(low_arousal_words))\n",
    "print('Some examples: ' + low_arousal_words.iloc[1]['Word'], low_arousal_words.iloc[3]['Word'], low_arousal_words.iloc[6]['Word'], sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(low_arousal_words['V.Mean.Sum']))\n",
    "sad_from_aro = low_arousal_words[low_arousal_words['V.Mean.Sum'] < 5.55]\n",
    "relaxed_from_aro = low_arousal_words[low_arousal_words['V.Mean.Sum'] > 5.55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(high_arousal_words['V.Mean.Sum']))\n",
    "angry_from_aro = high_arousal_words[high_arousal_words['V.Mean.Sum'] < 4.43]\n",
    "happy_from_aro = high_arousal_words[high_arousal_words['V.Mean.Sum'] > 4.43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно разбивать сильнее! например особенно эта группа!))\n",
    "# happy_from_aro \n",
    "# и angry я бы назвала agressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of SAD: 211\n"
     ]
    }
   ],
   "source": [
    "sad_words = sad_from_val['Word'].tolist()\n",
    "sad_words.extend(sad_from_aro['Word'].tolist())\n",
    "print('Numer of SAD:', len(sad_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of HAPPY: 144\n"
     ]
    }
   ],
   "source": [
    "happy_words = happy_from_val['Word'].tolist()\n",
    "happy_words.extend(happy_from_aro['Word'].tolist())\n",
    "print('Numer of HAPPY:', len(happy_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of ANGRY: 123\n"
     ]
    }
   ],
   "source": [
    "angry_words = angry_from_val['Word'].tolist()\n",
    "angry_words.extend(angry_from_aro['Word'].tolist())\n",
    "angry_words = list(set(angry_words))\n",
    "print('Numer of ANGRY:', len(angry_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of RELAXED: 202\n"
     ]
    }
   ],
   "source": [
    "relaxed_words = relaxed_from_val['Word'].tolist()\n",
    "relaxed_words.extend(relaxed_from_aro['Word'].tolist())\n",
    "relaxed_words = list(set(relaxed_words))\n",
    "print('Numer of RELAXED:', len(relaxed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# еще хорошо бы убедиться, что одни и те же слова не попали в несколько групп"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ad1c1dfd0a4590813cc0b0187d2c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee89269b3604bf3819027e162d95951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a922fabd994a95a511b46a4a938103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "emb_model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtoken_vectors(sent):\n",
    "    sent_marked = \"[CLS] \" + sent + \" [SEP]\"\n",
    "    tokenized = tokenizer.tokenize(sent_marked)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    segments_ids = [1] * len(tokenized)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = emb_model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # delete batch dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # move tokens to first dimension\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_cat = []\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "    return tokenized, token_vecs_cat\n",
    "\n",
    "def get_subtokens_to_join(tokenized):\n",
    "    to_join = []\n",
    "    join = []\n",
    "    for i, token in enumerate(tokenized):\n",
    "        if token.startswith('##'):\n",
    "            join.append(i)\n",
    "        else:\n",
    "            if join:\n",
    "                join.insert(0, join[0]-1)\n",
    "                to_join.append(join)\n",
    "            join = []\n",
    "    return to_join\n",
    "\n",
    "def get_full_token(part_for_join, tokenized):\n",
    "    full_token = ''\n",
    "    token_vecs = []\n",
    "    for sub_token in part_for_join:\n",
    "        full_token += tokenized[sub_token].replace('##', '')\n",
    "    return full_token\n",
    "\n",
    "def get_full_token_vectors(sent):\n",
    "    tokenized_sent, subtoken_vecs = get_subtoken_vectors(sent)\n",
    "    to_join = get_subtokens_to_join(tokenized_sent)\n",
    "    merged_join_list = []\n",
    "    for seq in to_join:\n",
    "        merged_join_list.extend(seq)\n",
    "        \n",
    "    join_list_ind = 0\n",
    "    token_vectors = []\n",
    "    token_words = []\n",
    "    skip = []\n",
    "    for i in range(len(subtoken_vecs)):\n",
    "        if i in skip:\n",
    "            continue\n",
    "        if i in merged_join_list:\n",
    "            join_indices = to_join[join_list_ind]\n",
    "            token_vec = [np.array(subtoken_vecs[ind]) for ind in join_indices]\n",
    "            full_token = get_full_token(join_indices, tokenized_sent)\n",
    "            token_vec_av = np.average(np.array(token_vec), axis=0)\n",
    "            skip.extend(join_indices)\n",
    "            join_list_ind += 1\n",
    "            token_vectors.append(token_vec_av)\n",
    "            token_words.append(full_token)\n",
    "        else:\n",
    "            token_vectors.append(subtoken_vecs[i].numpy())\n",
    "            token_words.append(tokenized_sent[i])\n",
    "    return token_words[1:-1], token_vectors[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word, vector = get_full_token_vectors('sadness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_vectors(emo_list):\n",
    "    emo_vectors = []\n",
    "    for word in emo_list:\n",
    "        tokens, vector = get_full_token_vectors(word)\n",
    "        if len(tokens) > 1:\n",
    "            print(tokens)\n",
    "            mean_vector = np.average(np.array(vector), axis=0)\n",
    "            emo_vectors.append(mean_vector)\n",
    "        else:\n",
    "            emo_vectors.append(vector[0])\n",
    "    return emo_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heart', 'disease']\n",
      "['breaking', 'and', 'entering']\n",
      "['drunk', 'driving']\n",
      "['measuring', 'tape']\n",
      "['t', '-', 'shirt']\n"
     ]
    }
   ],
   "source": [
    "angry_vecs = get_emotion_vectors(angry_words)\n",
    "sad_vecs = get_emotion_vectors(sad_words)\n",
    "happy_vecs = get_emotion_vectors(happy_words)\n",
    "relaxed_vecs = get_emotion_vectors(relaxed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "будет ли какая-то разница, если считать \"средний\" вектор всех грустных слов или посчитать отдельные и потом среднее посчитать уже у расстояний?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Mood</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Michael</td>\n",
       "      <td>I Want Your Sex</td>\n",
       "      <td>happy</td>\n",
       "      <td>[Verse 1]\\nIt’s late\\nTime for bed\\nSo I sit a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rob Zombie</td>\n",
       "      <td>Pussy Liquor</td>\n",
       "      <td>angry</td>\n",
       "      <td>Baby :\\nGimme a B\\nGimme a A\\nGimme a B\\nGimme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Katatonia</td>\n",
       "      <td>12</td>\n",
       "      <td>sad</td>\n",
       "      <td>[Chorus]\\nBlack theatre of love\\nViolet dancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bing Crosby</td>\n",
       "      <td>Swinging On A Star</td>\n",
       "      <td>happy</td>\n",
       "      <td>Would you like to swing on a star\\nCarry moonb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ludacris</td>\n",
       "      <td>Get Back</td>\n",
       "      <td>angry</td>\n",
       "      <td>[Intro]\\nHeads up! (woop, woop) Heads up! (woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Artist               Title   Mood  \\\n",
       "0  George Michael     I Want Your Sex  happy   \n",
       "1      Rob Zombie        Pussy Liquor  angry   \n",
       "2       Katatonia                  12    sad   \n",
       "3     Bing Crosby  Swinging On A Star  happy   \n",
       "4        Ludacris            Get Back  angry   \n",
       "\n",
       "                                                Text  \n",
       "0  [Verse 1]\\nIt’s late\\nTime for bed\\nSo I sit a...  \n",
       "1  Baby :\\nGimme a B\\nGimme a A\\nGimme a B\\nGimme...  \n",
       "2  [Chorus]\\nBlack theatre of love\\nViolet dancer...  \n",
       "3  Would you like to swing on a star\\nCarry moonb...  \n",
       "4  [Intro]\\nHeads up! (woop, woop) Heads up! (woo...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df = pd.read_csv('lyrics.csv')\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_1 = songs_df.iloc[0].Text\n",
    "# print(example_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляю пунктуацию - спорное решение учитывая что берт контекстный\n",
    "lines = example_text_1.split('\\n')\n",
    "lines = [re.sub('[{}]'.format(re.escape(punctuation)), '', line.lower()) for line in lines if not line.startswith('[') and line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "clean_lines = []\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_emo_dist(emo_list, vec):\n",
    "    dists = []\n",
    "    for emo_vec in emo_list:\n",
    "        dist = 1 - cosine(vec, emo_vec)\n",
    "        dists.append(dist)\n",
    "    return np.mean(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут не дистансы а симилярити......?\n",
    "def get_line_mood(line):\n",
    "    tokens, vectors = get_full_token_vectors(line)\n",
    "    sad, happy, angry, relaxed = [], [], [], []\n",
    "    for vec in vectors:\n",
    "        sad_mean = get_mean_emo_dist(sad_vecs, vec)\n",
    "        happy_mean = get_mean_emo_dist(happy_vecs, vec)\n",
    "        angry_mean = get_mean_emo_dist(angry_vecs, vec)\n",
    "        relaxed_mean = get_mean_emo_dist(relaxed_vecs, vec)\n",
    "        sad.append(sad_mean)\n",
    "        happy.append(happy_mean)\n",
    "        angry.append(angry_mean)\n",
    "        relaxed.append(relaxed_mean)\n",
    "    emos = ['sad', 'relaxed', 'happy', 'angry']\n",
    "    means = [np.mean(sad), np.mean(relaxed), np.mean(happy), np.mean(angry)]\n",
    "    return emos[np.argmax(means)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods = []\n",
    "for line in clean_lines:\n",
    "    moods.append(get_line_mood(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happy', 15), ('relaxed', 8), ('angry', 1)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(moods).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_2 = songs_df.iloc[14].Text\n",
    "# print(example_text_2)\n",
    "lines = example_text_2.split('\\n')\n",
    "lines = [re.sub('[{}]'.format(re.escape(punctuation)), '', line.lower()) for line in lines if not line.startswith('[') and line != '']\n",
    "clean_lines = []\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods = []\n",
    "for line in clean_lines:\n",
    "    moods.append(get_line_mood(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('angry', 47), ('happy', 10), ('relaxed', 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(moods).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_3 = songs_df.iloc[39].Text\n",
    "lines = example_text_3.split('\\n')\n",
    "lines = [re.sub('[{}]'.format(re.escape(punctuation)), '', line.lower()) for line in lines if not line.startswith('[') and line != '']\n",
    "clean_lines = []\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods = []\n",
    "for line in clean_lines:\n",
    "    moods.append(get_line_mood(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relaxed', 5), ('happy', 4), ('angry', 3)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(moods).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_4 = songs_df.iloc[78].Text\n",
    "lines = example_text_4.split('\\n')\n",
    "lines = [re.sub('[{}]'.format(re.escape(punctuation)), '', line.lower()) for line in lines if not line.startswith('[') and line != '']\n",
    "clean_lines = []\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yana\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Yana\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "moods = []\n",
    "for line in clean_lines:\n",
    "    moods.append(get_line_mood(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happy', 15), ('angry', 15), ('relaxed', 5), ('sad', 2)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(moods).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abduction',\n",
       " 'abusive',\n",
       " 'acne',\n",
       " 'afraid',\n",
       " 'AIDS',\n",
       " 'alcoholism',\n",
       " 'amputation',\n",
       " 'appendicitis',\n",
       " 'asbestos',\n",
       " 'blackmailer',\n",
       " 'bloodbath',\n",
       " 'cancer',\n",
       " 'castrate',\n",
       " 'castration',\n",
       " 'cellmate',\n",
       " 'chemo',\n",
       " 'chemotherapy',\n",
       " 'cirrhosis',\n",
       " 'coma',\n",
       " 'comatose',\n",
       " 'criminal',\n",
       " 'dead',\n",
       " 'deadly',\n",
       " 'deathbed',\n",
       " 'destitute',\n",
       " 'disastrous',\n",
       " 'dreaded',\n",
       " 'embolism',\n",
       " 'enemy',\n",
       " 'excrement',\n",
       " 'excruciating',\n",
       " 'extortion',\n",
       " 'fag',\n",
       " 'failure',\n",
       " 'feces',\n",
       " 'felon',\n",
       " 'flunk',\n",
       " 'fraud',\n",
       " 'funeral',\n",
       " 'gonorrhea',\n",
       " 'greedy',\n",
       " 'headache',\n",
       " 'helpless',\n",
       " 'HIV',\n",
       " 'homeless',\n",
       " 'hopeless',\n",
       " 'illness',\n",
       " 'incarceration',\n",
       " 'incest',\n",
       " 'incurable',\n",
       " 'infection',\n",
       " 'insecurity',\n",
       " 'insulting',\n",
       " 'jail',\n",
       " 'leprosy',\n",
       " 'lice',\n",
       " 'lifeless',\n",
       " 'lynching',\n",
       " 'menopause',\n",
       " 'misery',\n",
       " 'mistrust',\n",
       " 'moldy',\n",
       " 'molester',\n",
       " 'morgue',\n",
       " 'mortuary',\n",
       " 'motherless',\n",
       " 'mourning',\n",
       " 'mutilate',\n",
       " 'nausea',\n",
       " 'nauseous',\n",
       " 'neglect',\n",
       " 'negligent',\n",
       " 'overworked',\n",
       " 'parasitic',\n",
       " 'parkinsons',\n",
       " 'pedophile',\n",
       " 'perish',\n",
       " 'pimple',\n",
       " 'pollute',\n",
       " 'pollution',\n",
       " 'poverty',\n",
       " 'prison',\n",
       " 'puke',\n",
       " 'racist',\n",
       " 'repulsive',\n",
       " 'roach',\n",
       " 'rude',\n",
       " 'sabotage',\n",
       " 'sad',\n",
       " 'sadist',\n",
       " 'satanic',\n",
       " 'scum',\n",
       " 'seasick',\n",
       " 'slave',\n",
       " 'smallpox',\n",
       " 'steal',\n",
       " 'stomachache',\n",
       " 'stress',\n",
       " 'suffer',\n",
       " 'suicidal',\n",
       " 'syphilis',\n",
       " 'tapeworm',\n",
       " 'terrible',\n",
       " 'torture',\n",
       " 'traffic',\n",
       " 'tumor',\n",
       " 'unethical',\n",
       " 'unhappiness',\n",
       " 'unhappy',\n",
       " 'unsanitary',\n",
       " 'vandalism',\n",
       " 'vengeful',\n",
       " 'virus',\n",
       " 'vomit',\n",
       " 'worthless',\n",
       " 'wreck',\n",
       " 'aisle',\n",
       " 'axle',\n",
       " 'back',\n",
       " 'backwoods',\n",
       " 'ballpoint',\n",
       " 'banister',\n",
       " 'baseline',\n",
       " 'bland',\n",
       " 'boredom',\n",
       " 'bran',\n",
       " 'brick',\n",
       " 'brood',\n",
       " 'cardboard',\n",
       " 'cauliflower',\n",
       " 'chap',\n",
       " 'chimney',\n",
       " 'coal',\n",
       " 'conclude',\n",
       " 'confer',\n",
       " 'consulate',\n",
       " 'cordon',\n",
       " 'counter',\n",
       " 'decimeter',\n",
       " 'department',\n",
       " 'downfield',\n",
       " 'drab',\n",
       " 'dresser',\n",
       " 'dryer',\n",
       " 'dull',\n",
       " 'empty',\n",
       " 'fare',\n",
       " 'fiberglass',\n",
       " 'generic',\n",
       " 'genteel',\n",
       " 'gentry',\n",
       " 'geography',\n",
       " 'gerbil',\n",
       " 'geriatric',\n",
       " 'ground',\n",
       " 'hay',\n",
       " 'imprint',\n",
       " 'ivory',\n",
       " 'janitor',\n",
       " 'lack',\n",
       " 'lever',\n",
       " 'lint',\n",
       " 'localized',\n",
       " 'low',\n",
       " 'mantle',\n",
       " 'measuring tape',\n",
       " 'memo',\n",
       " 'mill',\n",
       " 'misty',\n",
       " 'monastery',\n",
       " 'moth',\n",
       " 'musk',\n",
       " 'mute',\n",
       " 'muted',\n",
       " 'noun',\n",
       " 'numb',\n",
       " 'occasional',\n",
       " 'opt',\n",
       " 'pail',\n",
       " 'pause',\n",
       " 'pediatrics',\n",
       " 'pew',\n",
       " 'postage',\n",
       " 'pouch',\n",
       " 'prairie',\n",
       " 'priesthood',\n",
       " 'repose',\n",
       " 'reverend',\n",
       " 'rowboat',\n",
       " 'schooner',\n",
       " 'seal',\n",
       " 'shallows',\n",
       " 'slab',\n",
       " 'sod',\n",
       " 'specific',\n",
       " 'steward',\n",
       " 'stool',\n",
       " 'strings',\n",
       " 'swab',\n",
       " 'syllable',\n",
       " 'synagogue',\n",
       " 'teaspoon',\n",
       " 'tenant',\n",
       " 'tiresome',\n",
       " 'tram',\n",
       " 'typewriter',\n",
       " 'uneventful',\n",
       " 'usage',\n",
       " 'whatnot',\n",
       " 'wimp',\n",
       " 'wool']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно работает далеко не идеально - часто выдает happy для песен, в которых я бы сама не смогла определить эмоцию. Мб уменьшать вес для happy? С сэд вообще очень печально - почти невозможно получить грустную песню. Мб связано со списком грустных слов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abduction',\n",
       " 'abusive',\n",
       " 'acne',\n",
       " 'afraid',\n",
       " 'AIDS',\n",
       " 'alcoholism',\n",
       " 'amputation',\n",
       " 'appendicitis',\n",
       " 'asbestos',\n",
       " 'blackmailer',\n",
       " 'bloodbath',\n",
       " 'cancer',\n",
       " 'castrate',\n",
       " 'castration',\n",
       " 'cellmate',\n",
       " 'chemo',\n",
       " 'chemotherapy',\n",
       " 'cirrhosis',\n",
       " 'coma',\n",
       " 'comatose',\n",
       " 'criminal',\n",
       " 'dead',\n",
       " 'deadly',\n",
       " 'deathbed',\n",
       " 'destitute',\n",
       " 'disastrous',\n",
       " 'dreaded',\n",
       " 'embolism',\n",
       " 'enemy',\n",
       " 'excrement',\n",
       " 'excruciating',\n",
       " 'extortion',\n",
       " 'fag',\n",
       " 'failure',\n",
       " 'feces',\n",
       " 'felon',\n",
       " 'flunk',\n",
       " 'fraud',\n",
       " 'funeral',\n",
       " 'gonorrhea',\n",
       " 'greedy',\n",
       " 'headache',\n",
       " 'helpless',\n",
       " 'HIV',\n",
       " 'homeless',\n",
       " 'hopeless',\n",
       " 'illness',\n",
       " 'incarceration',\n",
       " 'incest',\n",
       " 'incurable',\n",
       " 'infection',\n",
       " 'insecurity',\n",
       " 'insulting',\n",
       " 'jail',\n",
       " 'leprosy',\n",
       " 'lice',\n",
       " 'lifeless',\n",
       " 'lynching',\n",
       " 'menopause',\n",
       " 'misery',\n",
       " 'mistrust',\n",
       " 'moldy',\n",
       " 'molester',\n",
       " 'morgue',\n",
       " 'mortuary',\n",
       " 'motherless',\n",
       " 'mourning',\n",
       " 'mutilate',\n",
       " 'nausea',\n",
       " 'nauseous',\n",
       " 'neglect',\n",
       " 'negligent',\n",
       " 'overworked',\n",
       " 'parasitic',\n",
       " 'parkinsons',\n",
       " 'pedophile',\n",
       " 'perish',\n",
       " 'pimple',\n",
       " 'pollute',\n",
       " 'pollution',\n",
       " 'poverty',\n",
       " 'prison',\n",
       " 'puke',\n",
       " 'racist',\n",
       " 'repulsive',\n",
       " 'roach',\n",
       " 'rude',\n",
       " 'sabotage',\n",
       " 'sad',\n",
       " 'sadist',\n",
       " 'satanic',\n",
       " 'scum',\n",
       " 'seasick',\n",
       " 'slave',\n",
       " 'smallpox',\n",
       " 'steal',\n",
       " 'stomachache',\n",
       " 'stress',\n",
       " 'suffer',\n",
       " 'suicidal',\n",
       " 'syphilis',\n",
       " 'tapeworm',\n",
       " 'terrible',\n",
       " 'torture',\n",
       " 'traffic',\n",
       " 'tumor',\n",
       " 'unethical',\n",
       " 'unhappiness',\n",
       " 'unhappy',\n",
       " 'unsanitary',\n",
       " 'vandalism',\n",
       " 'vengeful',\n",
       " 'virus',\n",
       " 'vomit',\n",
       " 'worthless',\n",
       " 'wreck',\n",
       " 'aisle',\n",
       " 'axle',\n",
       " 'back',\n",
       " 'backwoods',\n",
       " 'ballpoint',\n",
       " 'banister',\n",
       " 'baseline',\n",
       " 'bland',\n",
       " 'boredom',\n",
       " 'bran',\n",
       " 'brick',\n",
       " 'brood',\n",
       " 'cardboard',\n",
       " 'cauliflower',\n",
       " 'chap',\n",
       " 'chimney',\n",
       " 'coal',\n",
       " 'conclude',\n",
       " 'confer',\n",
       " 'consulate',\n",
       " 'cordon',\n",
       " 'counter',\n",
       " 'decimeter',\n",
       " 'department',\n",
       " 'downfield',\n",
       " 'drab',\n",
       " 'dresser',\n",
       " 'dryer',\n",
       " 'dull',\n",
       " 'empty',\n",
       " 'fare',\n",
       " 'fiberglass',\n",
       " 'generic',\n",
       " 'genteel',\n",
       " 'gentry',\n",
       " 'geography',\n",
       " 'gerbil',\n",
       " 'geriatric',\n",
       " 'ground',\n",
       " 'hay',\n",
       " 'imprint',\n",
       " 'ivory',\n",
       " 'janitor',\n",
       " 'lack',\n",
       " 'lever',\n",
       " 'lint',\n",
       " 'localized',\n",
       " 'low',\n",
       " 'mantle',\n",
       " 'measuring tape',\n",
       " 'memo',\n",
       " 'mill',\n",
       " 'misty',\n",
       " 'monastery',\n",
       " 'moth',\n",
       " 'musk',\n",
       " 'mute',\n",
       " 'muted',\n",
       " 'noun',\n",
       " 'numb',\n",
       " 'occasional',\n",
       " 'opt',\n",
       " 'pail',\n",
       " 'pause',\n",
       " 'pediatrics',\n",
       " 'pew',\n",
       " 'postage',\n",
       " 'pouch',\n",
       " 'prairie',\n",
       " 'priesthood',\n",
       " 'repose',\n",
       " 'reverend',\n",
       " 'rowboat',\n",
       " 'schooner',\n",
       " 'seal',\n",
       " 'shallows',\n",
       " 'slab',\n",
       " 'sod',\n",
       " 'specific',\n",
       " 'steward',\n",
       " 'stool',\n",
       " 'strings',\n",
       " 'swab',\n",
       " 'syllable',\n",
       " 'synagogue',\n",
       " 'teaspoon',\n",
       " 'tenant',\n",
       " 'tiresome',\n",
       " 'tram',\n",
       " 'typewriter',\n",
       " 'uneventful',\n",
       " 'usage',\n",
       " 'whatnot',\n",
       " 'wimp',\n",
       " 'wool']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I used to laugh it off\\nI used to look the other way\\nI used to save them troubles for another day\\nI kept my fingers crossed\\nI used to never take the blame\\nI'd pull a sunshine story in the pouring rain\\nThe more I had to change, I just stayed the same\\nThe same, the same, I stayed the same\\nYou don't know what you've got, 'til you're missing it a lot\\nI had to go throw it away\\nI was wrong from the start\\nFrom the bottom of my heart, I apologize\\nWhat I did to you was hurtful\\nWhat I'm going through is hurtful\\nI used to buy my time\\nI used to beat around the bush\\nI'd rather give my ego another push\\nI used to be a fool\\nIt was a foolish game I played\\nAnd it's a fool's faith, common mistakes I made\\nOnce I had it right, it was all too late\\nToo late, too late, too little too late\\nYou don't know what you've got, 'til you're missing it a lot\\nI had to go throw it away\\nI was wrong from the start\\nFrom the bottom of my heart, I apologize\\nWhat I did to you was hurtful\\nWhat I'm going through is hurtful\\nIt is hurtful, it's hurtful\\nWhat I did to you\\nWhat I did to you\\nWhat I did to you was hurtful\\nWhat I'm going through is hurtful\\nI was wrong from the start\\nFrom the bottom of my heart, I apologize\\nWhat I did to you was hurtful\\nAnd what I'm going through is hurtful\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df.iloc[78].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваще-то должно быть грустным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Mood</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Michael</td>\n",
       "      <td>I Want Your Sex</td>\n",
       "      <td>happy</td>\n",
       "      <td>[Verse 1]\\nIt’s late\\nTime for bed\\nSo I sit a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rob Zombie</td>\n",
       "      <td>Pussy Liquor</td>\n",
       "      <td>angry</td>\n",
       "      <td>Baby :\\nGimme a B\\nGimme a A\\nGimme a B\\nGimme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Katatonia</td>\n",
       "      <td>12</td>\n",
       "      <td>sad</td>\n",
       "      <td>[Chorus]\\nBlack theatre of love\\nViolet dancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bing Crosby</td>\n",
       "      <td>Swinging On A Star</td>\n",
       "      <td>happy</td>\n",
       "      <td>Would you like to swing on a star\\nCarry moonb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ludacris</td>\n",
       "      <td>Get Back</td>\n",
       "      <td>angry</td>\n",
       "      <td>[Intro]\\nHeads up! (woop, woop) Heads up! (woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Reel Big Fish</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>happy</td>\n",
       "      <td>Start spreadin' the news\\nI'm leaving today\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Loverboy</td>\n",
       "      <td>Hot Girls In Love</td>\n",
       "      <td>happy</td>\n",
       "      <td>She's so young at heart\\nShe likes the pleasur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Judas Priest</td>\n",
       "      <td>Bullet Train</td>\n",
       "      <td>angry</td>\n",
       "      <td>[Verse 1]\\nSunrise showing every flaw\\nPaying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The Foreshadowing</td>\n",
       "      <td>The Wandering</td>\n",
       "      <td>sad</td>\n",
       "      <td>Our souls are not ready for life\\nWe're gonna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Erik Hassle</td>\n",
       "      <td>Hurtful</td>\n",
       "      <td>sad</td>\n",
       "      <td>I used to laugh it off\\nI used to look the oth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Artist               Title   Mood  \\\n",
       "0      George Michael     I Want Your Sex  happy   \n",
       "1          Rob Zombie        Pussy Liquor  angry   \n",
       "2           Katatonia                  12    sad   \n",
       "3         Bing Crosby  Swinging On A Star  happy   \n",
       "4            Ludacris            Get Back  angry   \n",
       "..                ...                 ...    ...   \n",
       "74      Reel Big Fish  New York, New York  happy   \n",
       "75           Loverboy   Hot Girls In Love  happy   \n",
       "76       Judas Priest        Bullet Train  angry   \n",
       "77  The Foreshadowing       The Wandering    sad   \n",
       "78        Erik Hassle             Hurtful    sad   \n",
       "\n",
       "                                                 Text  \n",
       "0   [Verse 1]\\nIt’s late\\nTime for bed\\nSo I sit a...  \n",
       "1   Baby :\\nGimme a B\\nGimme a A\\nGimme a B\\nGimme...  \n",
       "2   [Chorus]\\nBlack theatre of love\\nViolet dancer...  \n",
       "3   Would you like to swing on a star\\nCarry moonb...  \n",
       "4   [Intro]\\nHeads up! (woop, woop) Heads up! (woo...  \n",
       "..                                                ...  \n",
       "74  Start spreadin' the news\\nI'm leaving today\\nI...  \n",
       "75  She's so young at heart\\nShe likes the pleasur...  \n",
       "76  [Verse 1]\\nSunrise showing every flaw\\nPaying ...  \n",
       "77  Our souls are not ready for life\\nWe're gonna ...  \n",
       "78  I used to laugh it off\\nI used to look the oth...  \n",
       "\n",
       "[79 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df.head(79)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
